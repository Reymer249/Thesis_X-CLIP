03/25/2025 14:37:45 - INFO -   Weights & Biases logging enabled
03/25/2025 14:37:45 - INFO -   Weights & Biases logging initialized
Model path: /home/s3705609/X-CLIP/modules/ViT-B-32.pt
Model path 2: /home/s3705609/.cache/clip/ViT-B-32.pt
Loaded JIT model
03/25/2025 14:37:47 - INFO -   loading archive file /home/s3705609/X-CLIP/modules/cross-base
03/25/2025 14:37:47 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

03/25/2025 14:37:47 - INFO -   Weight doesn't exsits. /home/s3705609/X-CLIP/modules/cross-base/cross_pytorch_model.bin
03/25/2025 14:37:47 - WARNING -   Stage-One:True, Stage-Two:False
03/25/2025 14:37:47 - WARNING -   Test retrieval by loose type.
03/25/2025 14:37:47 - WARNING -   	 embed_dim: 512
03/25/2025 14:37:47 - WARNING -   	 image_resolution: 224
03/25/2025 14:37:47 - WARNING -   	 vision_layers: 12
03/25/2025 14:37:47 - WARNING -   	 vision_width: 768
03/25/2025 14:37:47 - WARNING -   	 vision_patch_size: 32
03/25/2025 14:37:47 - WARNING -   	 context_length: 77
03/25/2025 14:37:47 - WARNING -   	 vocab_size: 49408
03/25/2025 14:37:47 - WARNING -   	 transformer_width: 512
03/25/2025 14:37:47 - WARNING -   	 transformer_heads: 8
03/25/2025 14:37:47 - WARNING -   	 transformer_layers: 12
03/25/2025 14:37:47 - WARNING -   		 linear_patch: 2d
03/25/2025 14:37:47 - WARNING -   	 cut_top_layer: 0
03/25/2025 14:37:50 - WARNING -   	 sim_header: seqTransf
03/25/2025 14:38:01 - INFO -   --------------------
03/25/2025 14:38:01 - INFO -   Weights of XCLIP not initialized from pretrained model:
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
03/25/2025 14:38:01 - INFO -   Weights from pretrained model not used in XCLIP:
   clip.input_resolution
   clip.context_length
   clip.vocab_size
03/25/2025 14:38:01 - INFO -   Model has 260,258,738 trainable parameters and 27,736,320 frozen parameters
