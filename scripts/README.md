### Structure

###### folders
- generate_hard_negatives_positives - folder with scripts, which were used to generate json files with hard
negatives and positives
- train_02data_set20_proper_distr - scripts to train model using 0.2 data with the set of hard negatives and positives generated 
in a "proper" way to ensure the right distribution. We did not use this method in the paper, and did not include any results. You
may find the scripts to generate hard negatives and positives in a "proper" way in the `helpers_scripts` folder
- train_02data - scripts to train model with different settings using 0.2 data
- train_02data_LLM_sentences - scripts to train model with different settings using 0.2 data and LLM-generated hard 
positives and negatives
- train_02data_set05 - scripts to train model with different settings using 0.2 data and set of 
hard positives and negatives of the size 5
- train_02data_set20 - scripts to train model with different settings using 0.2 data and set of 
hard positives and negatives of the size 20

###### files
- `continue_run_xclip.sh` - script to continue the training when it gets interrupted. Very similar ro `run_xclip.sh`, but
loads the model on some epoch and continues from that.
- `extract_features.sh` - script which will extract features (embeddings) from a set of videos. We then use them to
calculate PosRank or Brittleness faster.
- `PosRank.sh` - script to calculate PosRank. Will output the txt file like this:

meanR of change filtered_vatex1k5_noun_RE20 is:0.252685824554963\
GT @ R1 count filtered_vatex1k5_noun_RE20 is:1221

Here, the first value is the PosRank.

- `PosRank_n_Brit.sh` - script to calculate PosRank and Brittleness. Will output the txt file like this:

meanR of change filtered_vatex1k5_noun_RE20 is:0.8781078312692057\
GT @ R1 count filtered_vatex1k5_noun_RE20 is:8121\
Brittleness: 0.1680789645218363 (1805/10739)

As synonyms are available in much smaller quantity for some parts of speech (e.g., for prepositions, 
comparing to nouns), we have less hard positives than negatives (i.e., in the set there are less captions for which
we have generated hard positives, than for which we have generated hard negatives; the paper for details). As for Brittleness we need
at least one hard negative and one hard positive per original caption, we have much less data points for Brittleness, than for PosRank, 
where we need hard negatives only for calculations. 

As it would be confusing to report values for one number of sentences for PosRank, while for another number 
for Brittleness, the results in the output are for the sentences with hard negatives and positives only.
Hereby, in the research we report results for Brittleness from this file, while for PosRank from the
one generated by `PosRank.sh`, as we evaluate more data there.

- `run_xclip.sh` - main script to run training
- `run_xclip.slurm` - main slurm script to run training
- `run_xclip_test.sh` - file to test the xclip because at some poit we realized that we tested it on the **test set**, 
even though we wanted to test on the **evaluation set** (for the reasons see the paper). Batch size 16.
- `run_xclip_test_b64.sh` - same as `run_xclip_test.sh`, but the batch size is 64