{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from argparse import Namespace\n",
    "from modules.tokenization_clip import SimpleTokenizer as ClipTokenizer\n",
    "from main_xclip import init_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-One:True, Stage-Two:False\n",
      "Test retrieval by loose type.\n",
      "\t embed_dim: 512\n",
      "\t image_resolution: 224\n",
      "\t vision_layers: 12\n",
      "\t vision_width: 768\n",
      "\t vision_patch_size: 32\n",
      "\t context_length: 77\n",
      "\t vocab_size: 49408\n",
      "\t transformer_width: 512\n",
      "\t transformer_heads: 8\n",
      "\t transformer_layers: 12\n",
      "\t cut_top_layer: 0\n",
      "\t sim_header: meanP\n",
      "[aac @ 0x5555a1673fc0] skip_data_stream_element: Input buffer exhausted before END element found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m text_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA cat is sleeping\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with actual text\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Prepare the inputs\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m video_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m input_ids, attention_mask \u001b[38;5;241m=\u001b[39m prepare_text(text_query, tokenizer)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Move tensors to device\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m, in \u001b[0;36mprepare_video\u001b[0;34m(video_path, num_frames, height, width)\u001b[0m\n\u001b[1;32m     34\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Stack frames into a single tensor [num_frames, channels, height, width]\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m video_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Reshape to match model input requirements [batch, pair, bs, ts, channels, height, width]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m video_tensor \u001b[38;5;241m=\u001b[39m video_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# Function to prepare video frames from a real video file\n",
    "def prepare_video(video_path, num_frames=8, height=224, width=224):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Select frames at regular intervals\n",
    "    frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n",
    "    \n",
    "    # Prepare transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((height, width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                             std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ])\n",
    "    \n",
    "    # Extract frames\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Convert to PIL Image\n",
    "            pil_img = Image.fromarray(frame)\n",
    "            # Apply transformations\n",
    "            img_tensor = transform(pil_img)\n",
    "            frames.append(img_tensor)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Stack frames into a single tensor [num_frames, channels, height, width]\n",
    "    video_tensor = torch.stack(frames)\n",
    "    \n",
    "    # Reshape to match model input requirements [batch, pair, bs, ts, channels, height, width]\n",
    "    video_tensor = video_tensor.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "    video_tensor = video_tensor.permute(0, 1, 2, 3, 4, 5, 6)\n",
    "    \n",
    "    return video_tensor\n",
    "\n",
    "# Function to prepare text input\n",
    "def prepare_text(text, tokenizer, max_words=20):\n",
    "    tokens = tokenizer.encode(text)  # Use the appropriate method to tokenize the text\n",
    "    tokens = tokens[:max_words]  # Truncate tokens to max_words\n",
    "    padding_length = max_words - len(tokens)\n",
    "    pad_token_id = 0  # Define a padding token ID (commonly 0 or another value)\n",
    "    tokens += [pad_token_id] * padding_length  # Pad tokens to max_words\n",
    "    attention_mask = [1] * len(tokens) + [0] * padding_length\n",
    "    return torch.tensor([tokens]), torch.tensor([attention_mask])\n",
    "\n",
    "# Main code\n",
    "args = Namespace(\n",
    "    # Required arguments\n",
    "    output_dir='./output',\n",
    "    cross_model='cross-base',\n",
    "    local_rank=0,\n",
    "    # Other necessary arguments with default values\n",
    "    task_type='retrieval',\n",
    "    datatype='msrvtt',\n",
    "    pretrained_clip_name='ViT-B/32',\n",
    "    max_words=20,\n",
    "    max_frames=8,\n",
    "    sim_header=\"meanP\",\n",
    "    loose_type=True,\n",
    "    n_gpu=1,\n",
    "    cache_dir='',\n",
    "    init_model=None,\n",
    ")\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = ClipTokenizer()\n",
    "\n",
    "# Initialize the model\n",
    "model = init_model(args, device, n_gpu, args.local_rank)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Configure the model to use meanP and loose_type=True\n",
    "model.sim_header = \"meanP\"\n",
    "model.loose_type = True\n",
    "\n",
    "# Path to your video file\n",
    "video_path = \"/home/s3705609/data1/VATEX/data/_-GgBjU0XMk_000050_000060.mp4\"  # Replace with actual path\n",
    "\n",
    "# Your text query\n",
    "text_query = \"A cat is sleeping\"  # Replace with actual text\n",
    "\n",
    "# Prepare the inputs\n",
    "video_tensor = prepare_video(video_path)\n",
    "input_ids, attention_mask = prepare_text(text_query, tokenizer)\n",
    "\n",
    "# Move tensors to device\n",
    "video_tensor = video_tensor.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "segment_ids = torch.zeros_like(input_ids).to(device)\n",
    "video_mask = torch.ones(1, args.max_frames).to(device)\n",
    "\n",
    "# Function to compute similarity\n",
    "def compute_similarity_manually(model, input_ids, segment_ids, input_mask, video, video_mask):\n",
    "    # First, get text features\n",
    "    sequence_hidden, seq_features = model.clip.encode_text(input_ids, return_hidden=True)\n",
    "    sequence_hidden, seq_features = sequence_hidden.float(), seq_features.float()\n",
    "    sequence_hidden = sequence_hidden.view(input_ids.size(0), -1, sequence_hidden.size(-1))\n",
    "    \n",
    "    # Reshape video for processing\n",
    "    b, pair, bs_vid, ts, c, h, w = video.shape\n",
    "    video_reshaped = video.view(b * pair * bs_vid * ts, c, h, w)\n",
    "    \n",
    "    # Get visual features\n",
    "    visual_hidden = model.clip.encode_image(video_reshaped).float()\n",
    "    visual_hidden = visual_hidden.view(b, -1, visual_hidden.size(-1))\n",
    "    \n",
    "    # Normalize features\n",
    "    sentence_output = sequence_hidden.squeeze(1)\n",
    "    sentence_output = sentence_output / sentence_output.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    video_output = visual_hidden\n",
    "    video_output = video_output / video_output.norm(dim=-1, keepdim=True)\n",
    "    video_mask_un = video_mask.to(dtype=torch.float).unsqueeze(-1)\n",
    "    video_output = video_output * video_mask_un\n",
    "    video_mask_un_sum = torch.sum(video_mask_un, dim=1, dtype=torch.float)\n",
    "    video_mask_un_sum[video_mask_un_sum == 0.] = 1.\n",
    "    video_output = torch.sum(video_output, dim=1) / video_mask_un_sum\n",
    "    video_output = video_output / video_output.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute simple cosine similarity\n",
    "    similarity = torch.matmul(sentence_output, video_output.t()) * model.clip.logit_scale.exp()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Compute similarity score\n",
    "with torch.no_grad():\n",
    "    similarity = compute_similarity_manually(model, input_ids, segment_ids, attention_mask, video_tensor, video_mask)\n",
    "    print(f\"Similarity score for '{text_query}':\", similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
