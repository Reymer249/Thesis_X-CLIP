nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0
g++ (GCC) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

## Available CUDA devices: 0
## Checking status of CUDA device with nvidia-smi
Thu Mar 20 18:37:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:62:00.0 Off |                  N/A |
| 30%   32C    P8             34W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[nltk_data] Downloading package wordnet to /home/s3705609/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
03/20/2025 18:37:26 - INFO -   Effective parameters:
03/20/2025 18:37:26 - INFO -     <<< batch_size: 16
03/20/2025 18:37:26 - INFO -     <<< batch_size_val: 64
03/20/2025 18:37:26 - INFO -     <<< cache_dir: 
03/20/2025 18:37:26 - INFO -     <<< coef_lr: 0.001
03/20/2025 18:37:26 - INFO -     <<< cross_model: cross-base
03/20/2025 18:37:26 - INFO -     <<< cross_num_hidden_layers: 4
03/20/2025 18:37:26 - INFO -     <<< data_path: /home/s3705609/data1/VATEX
03/20/2025 18:37:26 - INFO -     <<< datatype: vatex
03/20/2025 18:37:26 - INFO -     <<< do_eval: False
03/20/2025 18:37:26 - INFO -     <<< do_lower_case: False
03/20/2025 18:37:26 - INFO -     <<< do_pretrain: False
03/20/2025 18:37:26 - INFO -     <<< do_train: True
03/20/2025 18:37:26 - INFO -     <<< epochs: 5
03/20/2025 18:37:26 - INFO -     <<< eval_frame_order: 0
03/20/2025 18:37:26 - INFO -     <<< expand_msrvtt_sentences: False
03/20/2025 18:37:26 - INFO -     <<< feature_framerate: 1
03/20/2025 18:37:26 - INFO -     <<< features_path: /home/s3705609/data1/VATEX/clips
03/20/2025 18:37:26 - INFO -     <<< fp16: True
03/20/2025 18:37:26 - INFO -     <<< fp16_opt_level: O1
03/20/2025 18:37:26 - INFO -     <<< freeze_layer_num: 0
03/20/2025 18:37:26 - INFO -     <<< gradient_accumulation_steps: 1
03/20/2025 18:37:26 - INFO -     <<< hard_negative_rate: 0.5
03/20/2025 18:37:26 - INFO -     <<< init_model: None
03/20/2025 18:37:26 - INFO -     <<< linear_patch: 2d
03/20/2025 18:37:26 - INFO -     <<< local_rank: 0
03/20/2025 18:37:26 - INFO -     <<< loose_type: True
03/20/2025 18:37:26 - INFO -     <<< lr: 0.0001
03/20/2025 18:37:26 - INFO -     <<< lr_decay: 0.9
03/20/2025 18:37:26 - INFO -     <<< margin: 0.1
03/20/2025 18:37:26 - INFO -     <<< max_frames: 12
03/20/2025 18:37:26 - INFO -     <<< max_words: 32
03/20/2025 18:37:26 - INFO -     <<< n_display: 50
03/20/2025 18:37:26 - INFO -     <<< n_gpu: 2
03/20/2025 18:37:26 - INFO -     <<< n_pair: 1
03/20/2025 18:37:26 - INFO -     <<< negative_weighting: 1
03/20/2025 18:37:26 - INFO -     <<< num_thread_reader: 2
03/20/2025 18:37:26 - INFO -     <<< output_dir: ckpts3/xclip_vatex_vit16
03/20/2025 18:37:26 - INFO -     <<< pretrained_clip_name: ViT-B/16
03/20/2025 18:37:26 - INFO -     <<< rank: 0
03/20/2025 18:37:26 - INFO -     <<< resume_model: None
03/20/2025 18:37:26 - INFO -     <<< sampled_use_mil: False
03/20/2025 18:37:26 - INFO -     <<< seed: 42
03/20/2025 18:37:26 - INFO -     <<< sim_header: seqTransf
03/20/2025 18:37:26 - INFO -     <<< slice_framepos: 2
03/20/2025 18:37:26 - INFO -     <<< task_type: retrieval
03/20/2025 18:37:26 - INFO -     <<< text_num_hidden_layers: 12
03/20/2025 18:37:26 - INFO -     <<< train_csv: data/.train.csv
03/20/2025 18:37:26 - INFO -     <<< train_frame_order: 0
03/20/2025 18:37:26 - INFO -     <<< use_mil: False
03/20/2025 18:37:26 - INFO -     <<< val_csv: data/.val.csv
03/20/2025 18:37:26 - INFO -     <<< video_dim: 1024
03/20/2025 18:37:26 - INFO -     <<< visual_num_hidden_layers: 12
03/20/2025 18:37:26 - INFO -     <<< warmup_proportion: 0.1
03/20/2025 18:37:26 - INFO -     <<< world_size: 1
03/20/2025 18:37:26 - INFO -   device: cuda:0 n_gpu: 1
03/20/2025 18:37:28 - INFO -   loading archive file /home/s3705609/X-CLIP/modules/cross-base
03/20/2025 18:37:28 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

03/20/2025 18:37:28 - INFO -   Weight doesn't exsits. /home/s3705609/X-CLIP/modules/cross-base/cross_pytorch_model.bin
03/20/2025 18:37:28 - WARNING -   Stage-One:True, Stage-Two:False
03/20/2025 18:37:28 - WARNING -   Test retrieval by loose type.
03/20/2025 18:37:28 - WARNING -   	 embed_dim: 512
03/20/2025 18:37:28 - WARNING -   	 image_resolution: 224
03/20/2025 18:37:28 - WARNING -   	 vision_layers: 12
03/20/2025 18:37:28 - WARNING -   	 vision_width: 768
03/20/2025 18:37:28 - WARNING -   	 vision_patch_size: 16
03/20/2025 18:37:28 - WARNING -   	 context_length: 77
03/20/2025 18:37:28 - WARNING -   	 vocab_size: 49408
03/20/2025 18:37:28 - WARNING -   	 transformer_width: 512
03/20/2025 18:37:28 - WARNING -   	 transformer_heads: 8
03/20/2025 18:37:28 - WARNING -   	 transformer_layers: 12
03/20/2025 18:37:28 - WARNING -   		 linear_patch: 2d
03/20/2025 18:37:28 - WARNING -   	 cut_top_layer: 0
03/20/2025 18:37:31 - WARNING -   	 sim_header: seqTransf
03/20/2025 18:37:42 - INFO -   --------------------
03/20/2025 18:37:42 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
03/20/2025 18:37:42 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
03/20/2025 18:37:54 - INFO -   ***** Running test *****
03/20/2025 18:37:54 - INFO -     Num examples = 60000
03/20/2025 18:37:54 - INFO -     Batch size = 64
03/20/2025 18:37:54 - INFO -     Num steps = 938
03/20/2025 18:37:54 - INFO -   ***** Running val *****
03/20/2025 18:37:54 - INFO -     Num examples = 30000
Total Paire: train 259910
Video number: 23778
Total Paire: train 259910
03/20/2025 18:38:13 - INFO -   ***** Running training *****
03/20/2025 18:38:13 - INFO -     Num examples = 259910
03/20/2025 18:38:13 - INFO -     Batch size = 16
03/20/2025 18:38:13 - INFO -     Num steps = 81220
Traceback (most recent call last):
  File "/home/s3705609/X-CLIP/main_xclip.py", line 553, in <module>
    main()
  File "/home/s3705609/X-CLIP/main_xclip.py", line 527, in main
    tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/main_xclip.py", line 267, in train_epoch
    loss = model(input_ids, segment_ids, input_mask, video, video_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/modeling_xclip.py", line 160, in forward
    (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/modeling_xclip.py", line 216, in get_sequence_visual_output
    visual_output = self.get_visual_output(video, video_mask, shaped=True, video_frame=video_frame)                  # [bs, num_frames, dim]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/modeling_xclip.py", line 198, in get_visual_output
    visual_hidden = self.clip.encode_image(video, video_frame=video_frame).float()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 451, in encode_image
    hidden = self.visual(image.type(self.dtype), video_frame=video_frame)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 314, in forward
    x = self.transformer(x, video_frame=video_frame)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 266, in forward
    return self.resblocks((x, video_frame))[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 254, in forward
    x = x + self.mlp(self.ln_2(x))
        ~~^~~~~~~~~~~~~~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 10.57 GiB of which 21.06 MiB is free. Including non-PyTorch memory, this process has 10.55 GiB memory in use. Of the allocated memory 10.14 GiB is allocated by PyTorch, and 59.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2025-03-20 18:39:23,734] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3085891) of binary: /data1/s3705609/venv_clip/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 810, in <module>
    main()
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-20_18:39:23
  host      : node851.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3085891)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
