nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0
g++ (GCC) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

## Available CUDA devices: 0,1
## Checking status of CUDA device with nvidia-smi
Fri Mar 21 16:12:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:62:00.0 Off |                  N/A |
| 29%   33C    P8             34W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:B9:00.0 Off |                  N/A |
| 30%   30C    P8             19W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
CUDA_VISIBLE_DEVICES: 0,1
MASTER_ADDR: localhost
MASTER_PORT: 15511
[nltk_data] Downloading package wordnet to /home/s3705609/nltk_data...
[nltk_data] Downloading package wordnet to /home/s3705609/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data]   Package wordnet is already up-to-date!
[Process 3202451] rank = 1, world_size = 2, local_rank = 1
[Process 3202450] rank = 0, world_size = 2, local_rank = 0
[Process 3202450] Initialized process group: rank=0, world_size=2
[Process 3202451] Initialized process group: rank=1, world_size=2
03/21/2025 16:13:05 - INFO -   CUDA_VISIBLE_DEVICES: 0,1
03/21/2025 16:13:05 - INFO -   Effective parameters:
03/21/2025 16:13:05 - INFO -   Total available GPUs: 2
03/21/2025 16:13:05 - INFO -     <<< batch_size: 16
03/21/2025 16:13:05 - INFO -   Process 3202451 using device: cuda:1, local_rank: 1
03/21/2025 16:13:05 - INFO -     <<< batch_size_val: 64
03/21/2025 16:13:05 - INFO -   Current device index: 1
03/21/2025 16:13:05 - INFO -     <<< cache_dir: 
03/21/2025 16:13:05 - INFO -     <<< coef_lr: 0.001
03/21/2025 16:13:05 - INFO -   Device properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10824MB, multi_processor_count=68)
03/21/2025 16:13:05 - INFO -     <<< cross_model: cross-base
03/21/2025 16:13:05 - INFO -     <<< cross_num_hidden_layers: 4
03/21/2025 16:13:05 - INFO -     <<< data_path: /home/s3705609/data1/VATEX
03/21/2025 16:13:05 - INFO -     <<< datatype: vatex
03/21/2025 16:13:05 - INFO -     <<< do_eval: False
03/21/2025 16:13:05 - INFO -     <<< do_lower_case: False
03/21/2025 16:13:05 - INFO -     <<< do_pretrain: False
03/21/2025 16:13:05 - INFO -     <<< do_train: True
03/21/2025 16:13:05 - INFO -     <<< epochs: 5
03/21/2025 16:13:05 - INFO -     <<< eval_frame_order: 0
03/21/2025 16:13:05 - INFO -     <<< expand_msrvtt_sentences: False
03/21/2025 16:13:05 - INFO -     <<< feature_framerate: 1
03/21/2025 16:13:05 - INFO -     <<< features_path: /home/s3705609/data1/VATEX/clips
03/21/2025 16:13:05 - INFO -     <<< fp16: True
03/21/2025 16:13:05 - INFO -     <<< fp16_opt_level: O2
03/21/2025 16:13:05 - INFO -     <<< freeze_layer_num: 0
03/21/2025 16:13:05 - INFO -     <<< gradient_accumulation_steps: 1
03/21/2025 16:13:05 - INFO -     <<< hard_negative_rate: 0.5
03/21/2025 16:13:05 - INFO -     <<< init_model: None
03/21/2025 16:13:05 - INFO -     <<< linear_patch: 2d
03/21/2025 16:13:05 - INFO -     <<< local_rank: 0
03/21/2025 16:13:05 - INFO -     <<< loose_type: True
03/21/2025 16:13:05 - INFO -     <<< lr: 0.0001
03/21/2025 16:13:05 - INFO -     <<< lr_decay: 0.9
03/21/2025 16:13:05 - INFO -     <<< margin: 0.1
03/21/2025 16:13:05 - INFO -     <<< max_frames: 12
03/21/2025 16:13:05 - INFO -     <<< max_words: 32
03/21/2025 16:13:05 - INFO -     <<< n_display: 50
03/21/2025 16:13:05 - INFO -     <<< n_gpu: 2
03/21/2025 16:13:05 - INFO -     <<< n_pair: 1
03/21/2025 16:13:05 - INFO -     <<< negative_weighting: 1
03/21/2025 16:13:05 - INFO -     <<< num_thread_reader: 2
03/21/2025 16:13:05 - INFO -     <<< output_dir: ckpts3/xclip_vatex_vit16
03/21/2025 16:13:05 - INFO -     <<< pretrained_clip_name: ViT-B/16
03/21/2025 16:13:05 - INFO -     <<< rank: 0
03/21/2025 16:13:05 - INFO -     <<< resume_model: None
03/21/2025 16:13:05 - INFO -     <<< sampled_use_mil: False
03/21/2025 16:13:05 - INFO -     <<< seed: 42
03/21/2025 16:13:05 - INFO -     <<< sim_header: seqTransf
03/21/2025 16:13:05 - INFO -     <<< slice_framepos: 2
03/21/2025 16:13:05 - INFO -     <<< task_type: retrieval
03/21/2025 16:13:05 - INFO -     <<< text_num_hidden_layers: 12
03/21/2025 16:13:05 - INFO -     <<< train_csv: data/.train.csv
03/21/2025 16:13:05 - INFO -     <<< train_frame_order: 0
03/21/2025 16:13:05 - INFO -     <<< use_mil: False
03/21/2025 16:13:05 - INFO -     <<< val_csv: data/.val.csv
03/21/2025 16:13:05 - INFO -     <<< video_dim: 1024
03/21/2025 16:13:05 - INFO -     <<< visual_num_hidden_layers: 12
03/21/2025 16:13:05 - INFO -     <<< warmup_proportion: 0.1
03/21/2025 16:13:05 - INFO -     <<< world_size: 2
03/21/2025 16:13:05 - INFO -   CUDA_VISIBLE_DEVICES: 0,1
03/21/2025 16:13:05 - INFO -   Total available GPUs: 2
03/21/2025 16:13:05 - INFO -   Process 3202450 using device: cuda:0, local_rank: 0
03/21/2025 16:13:05 - INFO -   Current device index: 0
03/21/2025 16:13:05 - INFO -   Device properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10824MB, multi_processor_count=68)
03/21/2025 16:13:08 - INFO -   loading archive file /home/s3705609/X-CLIP/modules/cross-base
03/21/2025 16:13:08 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

03/21/2025 16:13:08 - INFO -   Weight doesn't exsits. /home/s3705609/X-CLIP/modules/cross-base/cross_pytorch_model.bin
03/21/2025 16:13:08 - WARNING -   Stage-One:True, Stage-Two:False
03/21/2025 16:13:08 - WARNING -   Test retrieval by loose type.
03/21/2025 16:13:08 - WARNING -   	 embed_dim: 512
03/21/2025 16:13:08 - WARNING -   	 image_resolution: 224
03/21/2025 16:13:08 - WARNING -   	 vision_layers: 12
03/21/2025 16:13:08 - WARNING -   	 vision_width: 768
03/21/2025 16:13:08 - WARNING -   	 vision_patch_size: 16
03/21/2025 16:13:08 - WARNING -   	 context_length: 77
03/21/2025 16:13:08 - WARNING -   	 vocab_size: 49408
03/21/2025 16:13:08 - WARNING -   	 transformer_width: 512
03/21/2025 16:13:08 - WARNING -   	 transformer_heads: 8
03/21/2025 16:13:08 - WARNING -   	 transformer_layers: 12
03/21/2025 16:13:08 - WARNING -   		 linear_patch: 2d
03/21/2025 16:13:08 - WARNING -   	 cut_top_layer: 0
03/21/2025 16:13:10 - WARNING -   	 sim_header: seqTransf
03/21/2025 16:13:23 - INFO -   --------------------
03/21/2025 16:13:23 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
03/21/2025 16:13:23 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
03/21/2025 16:13:32 - INFO -   ***** Running test *****
03/21/2025 16:13:32 - INFO -     Num examples = 60000
03/21/2025 16:13:32 - INFO -     Batch size = 64
03/21/2025 16:13:32 - INFO -     Num steps = 938
03/21/2025 16:13:32 - INFO -   ***** Running val *****
03/21/2025 16:13:32 - INFO -     Num examples = 30000
Total Paire: train 259910
Video number: 23778
Total Paire: train 259910
Total Paire: train 259910
Video number: 23778
Total Paire: train 259910
03/21/2025 16:13:46 - INFO -   ***** Running training *****
03/21/2025 16:13:46 - INFO -     Num examples = 259910
03/21/2025 16:13:46 - INFO -     Batch size = 16
03/21/2025 16:13:46 - INFO -     Num steps = 81220
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Traceback (most recent call last):
  File "/home/s3705609/X-CLIP/main_xclip_work.py", line 608, in <module>
    main()
  File "/home/s3705609/X-CLIP/main_xclip_work.py", line 582, in main
    tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/main_xclip_work.py", line 318, in train_epoch
    for step, batch in enumerate(train_dataloader):
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1325, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
KeyError: Caught KeyError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/s3705609/X-CLIP/dataloaders/dataloader_vatex_neg_aug_word_phrase_retrieval.py", line 421, in __getitem__
    video, video_mask = self._get_rawvideo(choice_video_ids)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/dataloaders/dataloader_vatex_neg_aug_word_phrase_retrieval.py", line 365, in _get_rawvideo
    video_path = self.video_dict[video_id]
                 ~~~~~~~~~~~~~~~^^^^^^^^^^
KeyError: 'j80RGZR2Rzs_000011_000021'

[2025-03-21 16:15:27,344] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3202451 closing signal SIGTERM
[2025-03-21 16:15:27,512] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3202450) of binary: /data1/s3705609/venv_clip/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 810, in <module>
    main()
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip_work.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-21_16:15:27
  host      : node851.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3202450)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
