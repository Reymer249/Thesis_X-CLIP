nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0
g++ (GCC) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

## Available CUDA devices: 0
## Checking status of CUDA device with nvidia-smi
Fri Mar 21 14:56:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:B9:00.0 Off |                  N/A |
| 30%   29C    P8              1W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[nltk_data] Downloading package wordnet to /home/s3705609/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
03/21/2025 14:56:44 - INFO -   Effective parameters:
03/21/2025 14:56:44 - INFO -     <<< batch_size: 16
03/21/2025 14:56:44 - INFO -     <<< batch_size_val: 64
03/21/2025 14:56:44 - INFO -     <<< cache_dir: 
03/21/2025 14:56:44 - INFO -     <<< coef_lr: 0.001
03/21/2025 14:56:44 - INFO -     <<< cross_model: cross-base
03/21/2025 14:56:44 - INFO -     <<< cross_num_hidden_layers: 4
03/21/2025 14:56:44 - INFO -     <<< data_path: /home/s3705609/data1/VATEX
03/21/2025 14:56:44 - INFO -     <<< datatype: vatex
03/21/2025 14:56:44 - INFO -     <<< do_eval: False
03/21/2025 14:56:44 - INFO -     <<< do_lower_case: False
03/21/2025 14:56:44 - INFO -     <<< do_pretrain: False
03/21/2025 14:56:44 - INFO -     <<< do_train: True
03/21/2025 14:56:44 - INFO -     <<< epochs: 5
03/21/2025 14:56:44 - INFO -     <<< eval_frame_order: 0
03/21/2025 14:56:44 - INFO -     <<< expand_msrvtt_sentences: False
03/21/2025 14:56:44 - INFO -     <<< feature_framerate: 1
03/21/2025 14:56:44 - INFO -     <<< features_path: /home/s3705609/data1/VATEX/clips
03/21/2025 14:56:44 - INFO -     <<< fp16: True
03/21/2025 14:56:44 - INFO -     <<< fp16_opt_level: O1
03/21/2025 14:56:44 - INFO -     <<< freeze_layer_num: 0
03/21/2025 14:56:44 - INFO -     <<< gradient_accumulation_steps: 1
03/21/2025 14:56:44 - INFO -     <<< hard_negative_rate: 0.5
03/21/2025 14:56:44 - INFO -     <<< init_model: None
03/21/2025 14:56:44 - INFO -     <<< linear_patch: 2d
03/21/2025 14:56:44 - INFO -     <<< local_rank: 0
03/21/2025 14:56:44 - INFO -     <<< loose_type: True
03/21/2025 14:56:44 - INFO -     <<< lr: 0.0001
03/21/2025 14:56:44 - INFO -     <<< lr_decay: 0.9
03/21/2025 14:56:44 - INFO -     <<< margin: 0.1
03/21/2025 14:56:44 - INFO -     <<< max_frames: 12
03/21/2025 14:56:44 - INFO -     <<< max_words: 32
03/21/2025 14:56:44 - INFO -     <<< n_display: 50
03/21/2025 14:56:44 - INFO -     <<< n_gpu: 1
03/21/2025 14:56:44 - INFO -     <<< n_pair: 1
03/21/2025 14:56:44 - INFO -     <<< negative_weighting: 1
03/21/2025 14:56:44 - INFO -     <<< num_thread_reader: 1
03/21/2025 14:56:44 - INFO -     <<< output_dir: ckpts3/xclip_vatex_vit16
03/21/2025 14:56:44 - INFO -     <<< pretrained_clip_name: ViT-B/16
03/21/2025 14:56:44 - INFO -     <<< rank: 0
03/21/2025 14:56:44 - INFO -     <<< resume_model: None
03/21/2025 14:56:44 - INFO -     <<< sampled_use_mil: False
03/21/2025 14:56:44 - INFO -     <<< seed: 42
03/21/2025 14:56:44 - INFO -     <<< sim_header: seqTransf
03/21/2025 14:56:44 - INFO -     <<< slice_framepos: 2
03/21/2025 14:56:44 - INFO -     <<< task_type: retrieval
03/21/2025 14:56:44 - INFO -     <<< text_num_hidden_layers: 12
03/21/2025 14:56:44 - INFO -     <<< train_csv: data/.train.csv
03/21/2025 14:56:44 - INFO -     <<< train_frame_order: 0
03/21/2025 14:56:44 - INFO -     <<< use_mil: False
03/21/2025 14:56:44 - INFO -     <<< val_csv: data/.val.csv
03/21/2025 14:56:44 - INFO -     <<< video_dim: 1024
03/21/2025 14:56:44 - INFO -     <<< visual_num_hidden_layers: 12
03/21/2025 14:56:44 - INFO -     <<< warmup_proportion: 0.1
03/21/2025 14:56:44 - INFO -     <<< world_size: 1
_CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10824MB, multi_processor_count=68)
03/21/2025 14:56:44 - INFO -   device: cuda:0 n_gpu: 1
03/21/2025 14:56:46 - INFO -   loading archive file /home/s3705609/X-CLIP/modules/cross-base
03/21/2025 14:56:46 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

03/21/2025 14:56:46 - INFO -   Weight doesn't exsits. /home/s3705609/X-CLIP/modules/cross-base/cross_pytorch_model.bin
03/21/2025 14:56:46 - WARNING -   Stage-One:True, Stage-Two:False
03/21/2025 14:56:46 - WARNING -   Test retrieval by loose type.
03/21/2025 14:56:46 - WARNING -   	 embed_dim: 512
03/21/2025 14:56:46 - WARNING -   	 image_resolution: 224
03/21/2025 14:56:46 - WARNING -   	 vision_layers: 12
03/21/2025 14:56:46 - WARNING -   	 vision_width: 768
03/21/2025 14:56:46 - WARNING -   	 vision_patch_size: 16
03/21/2025 14:56:46 - WARNING -   	 context_length: 77
03/21/2025 14:56:46 - WARNING -   	 vocab_size: 49408
03/21/2025 14:56:46 - WARNING -   	 transformer_width: 512
03/21/2025 14:56:46 - WARNING -   	 transformer_heads: 8
03/21/2025 14:56:46 - WARNING -   	 transformer_layers: 12
03/21/2025 14:56:46 - WARNING -   		 linear_patch: 2d
03/21/2025 14:56:46 - WARNING -   	 cut_top_layer: 0
03/21/2025 14:56:50 - WARNING -   	 sim_header: seqTransf
03/21/2025 14:57:01 - INFO -   --------------------
03/21/2025 14:57:01 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
03/21/2025 14:57:01 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
03/21/2025 14:57:12 - INFO -   ***** Running test *****
03/21/2025 14:57:12 - INFO -     Num examples = 60000
03/21/2025 14:57:12 - INFO -     Batch size = 64
03/21/2025 14:57:12 - INFO -     Num steps = 938
03/21/2025 14:57:12 - INFO -   ***** Running val *****
03/21/2025 14:57:12 - INFO -     Num examples = 30000
Total Paire: train 259910
Video number: 23778
Total Paire: train 259910
node855:111431:111431 [0] NCCL INFO Bootstrap : Using eth01:10.161.19.87<0>
node855:111431:111431 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
node855:111431:111431 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
node855:111431:111431 [0] NCCL INFO cudaDriverVersion 12060
NCCL version 2.18.3+cuda12.1
node855:111431:111431 [0] NCCL INFO init.cc:1584 Cuda Host Alloc Size 4 pointer 0x155473600000
node855:111431:113108 [0] NCCL INFO NET/IB : No device found.
node855:111431:113108 [0] NCCL INFO NET/Socket : Using [0]eth01:10.161.19.87<0> [1]ens9f1:10.161.27.87<0>
node855:111431:113108 [0] NCCL INFO Using network Socket
node855:111431:113108 [0] NCCL INFO comm 0xa9d40c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId b9000 commId 0x2e47b4a2da58b6a5 - Init START
node855:111431:113108 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 0 'eth01'
node855:111431:113108 [0] NCCL INFO NET/Socket : GPU Direct RDMA Disabled for HCA 1 'ens9f1'
node855:111431:113108 [0] NCCL INFO === System : maxBw 5000.0 totalBw 0.0 ===
node855:111431:113108 [0] NCCL INFO CPU/1 (1/1/2)
node855:111431:113108 [0] NCCL INFO + PCI[12.0] - PCI/AF000 (10b5879610138796)
node855:111431:113108 [0] NCCL INFO               + PCI[12.0] - GPU/B9000 (0)
node855:111431:113108 [0] NCCL INFO + SYS[10.0] - CPU/0
node855:111431:113108 [0] NCCL INFO CPU/0 (1/1/2)
node855:111431:113108 [0] NCCL INFO + SYS[10.0] - CPU/1
node855:111431:113108 [0] NCCL INFO + PCI[3.0] - NIC/27000
node855:111431:113108 [0] NCCL INFO ==========================================
node855:111431:113108 [0] NCCL INFO GPU/B9000 :GPU/B9000 (0/5000.000000/LOC) CPU/1 (2/12.000000/PHB) CPU/0 (3/10.000000/SYS) 
node855:111431:113108 [0] NCCL INFO Setting affinity for GPU 0 to 0200,00020000
node855:111431:113108 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
node855:111431:113108 [0] NCCL INFO  0 : GPU/0
node855:111431:113108 [0] NCCL INFO  1 : GPU/0
node855:111431:113108 [0] NCCL INFO  2 : GPU/0
node855:111431:113108 [0] NCCL INFO  3 : GPU/0
node855:111431:113108 [0] NCCL INFO  4 : GPU/0
node855:111431:113108 [0] NCCL INFO  5 : GPU/0
node855:111431:113108 [0] NCCL INFO  6 : GPU/0
node855:111431:113108 [0] NCCL INFO  7 : GPU/0
node855:111431:113108 [0] NCCL INFO  8 : GPU/0
node855:111431:113108 [0] NCCL INFO  9 : GPU/0
node855:111431:113108 [0] NCCL INFO 10 : GPU/0
node855:111431:113108 [0] NCCL INFO 11 : GPU/0
node855:111431:113108 [0] NCCL INFO 12 : GPU/0
node855:111431:113108 [0] NCCL INFO 13 : GPU/0
node855:111431:113108 [0] NCCL INFO 14 : GPU/0
node855:111431:113108 [0] NCCL INFO 15 : GPU/0
node855:111431:113108 [0] NCCL INFO Pattern 3, crossNic 0, nChannels 16, bw 40.000000/40.000000, type LOC/PIX, sameChannels 1
node855:111431:113108 [0] NCCL INFO  0 : GPU/0
node855:111431:113108 [0] NCCL INFO  1 : GPU/0
node855:111431:113108 [0] NCCL INFO  2 : GPU/0
node855:111431:113108 [0] NCCL INFO  3 : GPU/0
node855:111431:113108 [0] NCCL INFO  4 : GPU/0
node855:111431:113108 [0] NCCL INFO  5 : GPU/0
node855:111431:113108 [0] NCCL INFO  6 : GPU/0
node855:111431:113108 [0] NCCL INFO  7 : GPU/0
node855:111431:113108 [0] NCCL INFO  8 : GPU/0
node855:111431:113108 [0] NCCL INFO  9 : GPU/0
node855:111431:113108 [0] NCCL INFO 10 : GPU/0
node855:111431:113108 [0] NCCL INFO 11 : GPU/0
node855:111431:113108 [0] NCCL INFO 12 : GPU/0
node855:111431:113108 [0] NCCL INFO 13 : GPU/0
node855:111431:113108 [0] NCCL INFO 14 : GPU/0
node855:111431:113108 [0] NCCL INFO 15 : GPU/0
node855:111431:113108 [0] NCCL INFO Tree 0 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 16 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 1 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 17 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 2 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 18 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 3 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 19 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 4 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 20 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 5 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 21 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 6 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 22 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 7 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 23 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 8 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 24 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 9 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 25 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 10 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 26 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 11 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 27 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 12 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 28 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 13 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 29 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 14 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 30 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 15 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Tree 31 : -1 -> 0 -> -1/-1/-1
node855:111431:113108 [0] NCCL INFO Channel 00/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 01/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 02/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 03/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 04/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 05/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 06/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 07/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 08/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 09/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 10/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 11/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 12/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 13/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 14/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 15/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 16/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 17/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 18/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 19/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 20/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 21/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 22/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 23/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 24/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 25/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 26/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 27/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 28/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 29/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 30/32 :    0
node855:111431:113108 [0] NCCL INFO Channel 31/32 :    0
node855:111431:113108 [0] NCCL INFO Ring 00 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 01 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 02 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 03 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 04 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 05 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 06 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 07 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 08 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 09 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 10 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 11 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 12 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 13 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 14 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 15 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 16 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 17 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 18 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 19 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 20 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 21 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 22 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 23 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 24 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 25 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 26 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 27 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 28 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 29 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 30 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Ring 31 : 0 -> 0 -> 0
node855:111431:113108 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
node855:111431:113108 [0] NCCL INFO P2P Chunksize set to 131072
node855:111431:113108 [0] NCCL INFO misc/utils.cc:235 memory stack hunk malloc(65536)
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa00000
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa00200
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa00400
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa00600
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa00800
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa00a00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa00c00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa00e00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa01000
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa01200
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa01400
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa01600
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa01800
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa01a00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa01c00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa01e00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa02000
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa02200
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa02400
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa02600
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa02800
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa02a00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa02c00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa02e00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa03000
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa03200
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa03400
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa03600
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa03800
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa03a00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa03c00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa03e00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa04000
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa04200
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa04400
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa04600
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa04800
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa04a00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa04c00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa04e00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa05000
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa05200
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa05400
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa05600
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa05800
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa05a00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa05c00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa05e00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa06000
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa06200
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa06400
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa06600
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa06800
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa06a00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa06c00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa06e00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa07000
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa07200
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa07400
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa07600
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa07800
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa07a00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa07c00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa07e00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa08000
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa08200
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa08400
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa08600
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa08800
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa08a00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa08c00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa08e00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa09000
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa09200
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa09400
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa09600
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa09800
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa09a00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa09c00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa09e00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa0a000
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa0a200
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa0a400
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa0a600
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa0a800
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa0aa00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa0ac00
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa0ae00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa0b000
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa0b200
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa0b400
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa0b600
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa0b800
node855:111431:113108 [0] NCCL INFO channel.cc:40 Cuda Alloc Size 384 pointer 0x15545fa0ba00
node855:111431:113108 [0] NCCL INFO channel.cc:43 Cuda Alloc Size 24 pointer 0x15545fa0bc00
node855:111431:113108 [0] NCCL INFO channel.cc:52 Cuda Alloc Size 4 pointer 0x15545fa0be00
node855:111431:113108 [0] NCCL INFO Connected all rings
node855:111431:113108 [0] NCCL INFO Connected all trees
node855:111431:113108 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
node855:111431:113133 [0] NCCL INFO Mem Realloc old size 0, new size 8 pointer 0x155444004f20
node855:111431:113133 [0] NCCL INFO Allocated 4194660 bytes of shared memory in /dev/shm/nccl-6ekQ5V
node855:111431:113133 [0] NCCL INFO New proxy send connection 0 from local rank 0, transport 2
node855:111431:113133 [0] NCCL INFO proxyProgressAsync opId=0x15543de06340 op.type=1 op.reqBuff=0x155444000ba0 op.respSize=16 done
node855:111431:113108 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x15543de06340
node855:111431:113108 [0] NCCL INFO recvOpId=0x15543de06340 matches expected opId=0x15543de06340
node855:111431:113133 [0] NCCL INFO Received and initiated operation=Init res=0
node855:111431:113108 [0] NCCL INFO Connection to proxy localRank 0 -> connection 0x155444004f40
node855:111431:113133 [0] NCCL INFO transport/net.cc:446 Cuda Alloc Size 67108864 pointer 0x155438000000
node855:111431:113133 [0] NCCL INFO proxyProgressAsync opId=0x15543de06340 op.type=2 op.reqBuff=0x155444007c30 op.respSize=0 done
node855:111431:113108 [0] NCCL INFO ncclPollProxyResponse Received new opId=0x15543de06340
node855:111431:113133 [0] NCCL INFO Received and initiated operation=SharedInit res=0
node855:111431:113108 [0] NCCL INFO recvOpId=0x15543de06340 matches expected opId=0x15543de06340
node855:111431:113108 [0] NCCL INFO init.cc:387 Cuda Alloc Size 7728 pointer 0x15545fa0c000
node855:111431:113108 [0] NCCL INFO init.cc:412 Cuda Host Alloc Size 33554432 pointer 0x155436000000
node855:111431:113108 [0] NCCL INFO init.cc:418 Cuda Host Alloc Size 128 pointer 0x155473600200
node855:111431:113108 [0] NCCL INFO comm 0xa9d40c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId b9000 commId 0x2e47b4a2da58b6a5 - Init COMPLETE
node855:111431:111431 [0] NCCL INFO AllGather: opCount 0 sendbuff 0x15548ddffe00 recvbuff 0x1554523c3200 count 8 datatype 0 op 0 root 0 comm 0xa9d40c0 [nranks=1] stream 0x8c219e0
node855:111431:111431 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x1554523c3200 recvbuff 0x1554523c3200 count 7584 datatype 0 op 0 root 0 comm 0xa9d40c0 [nranks=1] stream 0x8c219e0
node855:111431:111431 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x15542c000000 recvbuff 0x15542c000000 count 154922692 datatype 0 op 0 root 0 comm 0xa9d40c0 [nranks=1] stream 0x8c219e0
node855:111431:111431 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x15541c000000 recvbuff 0x15541c000000 count 248133632 datatype 0 op 0 root 0 comm 0xa9d40c0 [nranks=1] stream 0x8c219e0
03/21/2025 14:57:32 - INFO -   ***** Running training *****
03/21/2025 14:57:32 - INFO -     Num examples = 259910
03/21/2025 14:57:32 - INFO -     Batch size = 16
03/21/2025 14:57:32 - INFO -     Num steps = 81220
Traceback (most recent call last):
  File "/home/s3705609/X-CLIP/main_xclip.py", line 549, in <module>
    main()
  File "/home/s3705609/X-CLIP/main_xclip.py", line 523, in main
    tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/main_xclip.py", line 267, in train_epoch
    loss = model(input_ids, segment_ids, input_mask, video, video_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/modeling_xclip.py", line 160, in forward
    (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/modeling_xclip.py", line 216, in get_sequence_visual_output
    visual_output = self.get_visual_output(video, video_mask, shaped=True, video_frame=video_frame)                  # [bs, num_frames, dim]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/modeling_xclip.py", line 198, in get_visual_output
    visual_hidden = self.clip.encode_image(video, video_frame=video_frame).float()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 451, in encode_image
    hidden = self.visual(image.type(self.dtype), video_frame=video_frame)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 314, in forward
    x = self.transformer(x, video_frame=video_frame)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 266, in forward
    return self.resblocks((x, video_frame))[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/modules/module_clip.py", line 254, in forward
    x = x + self.mlp(self.ln_2(x))
        ~~^~~~~~~~~~~~~~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacty of 10.57 GiB of which 21.06 MiB is free. Including non-PyTorch memory, this process has 10.55 GiB memory in use. Of the allocated memory 10.14 GiB is allocated by PyTorch, and 59.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2025-03-21 14:58:15,898] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 111431) of binary: /data1/s3705609/venv_clip/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 810, in <module>
    main()
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-21_14:58:15
  host      : node855.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 111431)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
