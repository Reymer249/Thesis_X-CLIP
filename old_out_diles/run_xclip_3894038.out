nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0
g++ (GCC) 12.3.0
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

## Available CUDA devices: 0,1
## Checking status of CUDA device with nvidia-smi
Fri Mar 21 15:57:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:5A:00.0 Off |                  N/A |
| 30%   32C    P8             21W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:B9:00.0 Off |                  N/A |
| 30%   29C    P8              1W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[nltk_data] Downloading package wordnet to /home/s3705609/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package wordnet to /home/s3705609/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[Process 188822] rank = 1, world_size = 2, local_rank = 1
[Process 188821] rank = 0, world_size = 2, local_rank = 0
[Process 188821] Initialized process group: rank=0, world_size=2
[Process 188822] Initialized process group: rank=1, world_size=2
node855:188821:188821 [0] NCCL INFO Bootstrap : Using eth01:10.161.19.87<0>
node855:188821:188821 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
node855:188821:188821 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
node855:188821:188821 [0] NCCL INFO cudaDriverVersion 12060
NCCL version 2.18.3+cuda12.1
node855:188822:188822 [1] NCCL INFO cudaDriverVersion 12060
node855:188822:188822 [1] NCCL INFO Bootstrap : Using eth01:10.161.19.87<0>
node855:188822:188822 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
node855:188822:188822 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
node855:188821:188866 [0] NCCL INFO NET/IB : No device found.
node855:188821:188866 [0] NCCL INFO NET/Socket : Using [0]eth01:10.161.19.87<0> [1]ens9f1:10.161.27.87<0>
node855:188821:188866 [0] NCCL INFO Using network Socket
node855:188822:188867 [1] NCCL INFO NET/IB : No device found.
node855:188822:188867 [1] NCCL INFO NET/Socket : Using [0]eth01:10.161.19.87<0> [1]ens9f1:10.161.27.87<0>
node855:188822:188867 [1] NCCL INFO Using network Socket
node855:188821:188866 [0] NCCL INFO comm 0xbb3e620 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5a000 commId 0x389e581cdf93c33f - Init START
node855:188822:188867 [1] NCCL INFO comm 0xbb3d710 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId b9000 commId 0x389e581cdf93c33f - Init START
node855:188822:188867 [1] NCCL INFO Setting affinity for GPU 1 to 0300,00030000
node855:188821:188866 [0] NCCL INFO Setting affinity for GPU 0 to 06000006
node855:188822:188867 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
node855:188822:188867 [1] NCCL INFO P2P Chunksize set to 131072
node855:188821:188866 [0] NCCL INFO Channel 00/02 :    0   1
node855:188821:188866 [0] NCCL INFO Channel 01/02 :    0   1
node855:188821:188866 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
node855:188821:188866 [0] NCCL INFO P2P Chunksize set to 131072
node855:188822:188867 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
node855:188822:188867 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
node855:188821:188866 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
node855:188821:188866 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
node855:188822:188867 [1] NCCL INFO Connected all rings
node855:188822:188867 [1] NCCL INFO Connected all trees
node855:188822:188867 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
node855:188822:188867 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
node855:188821:188866 [0] NCCL INFO Connected all rings
node855:188821:188866 [0] NCCL INFO Connected all trees
node855:188821:188866 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
node855:188821:188866 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
node855:188822:188867 [1] NCCL INFO comm 0xbb3d710 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId b9000 commId 0x389e581cdf93c33f - Init COMPLETE
node855:188821:188866 [0] NCCL INFO comm 0xbb3e620 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5a000 commId 0x389e581cdf93c33f - Init COMPLETE
03/21/2025 15:57:44 - INFO -   Effective parameters:
03/21/2025 15:57:44 - INFO -     <<< batch_size: 16
03/21/2025 15:57:44 - INFO -     <<< batch_size_val: 64
03/21/2025 15:57:44 - INFO -     <<< cache_dir: 
03/21/2025 15:57:44 - INFO -     <<< coef_lr: 0.001
03/21/2025 15:57:44 - INFO -     <<< cross_model: cross-base
03/21/2025 15:57:44 - INFO -     <<< cross_num_hidden_layers: 4
03/21/2025 15:57:44 - INFO -     <<< data_path: /home/s3705609/data1/VATEX
03/21/2025 15:57:44 - INFO -     <<< datatype: vatex
03/21/2025 15:57:44 - INFO -     <<< do_eval: False
03/21/2025 15:57:44 - INFO -     <<< do_lower_case: False
03/21/2025 15:57:44 - INFO -     <<< do_pretrain: False
03/21/2025 15:57:44 - INFO -     <<< do_train: True
03/21/2025 15:57:44 - INFO -     <<< epochs: 5
03/21/2025 15:57:44 - INFO -     <<< eval_frame_order: 0
03/21/2025 15:57:44 - INFO -     <<< expand_msrvtt_sentences: False
03/21/2025 15:57:44 - INFO -     <<< feature_framerate: 1
03/21/2025 15:57:44 - INFO -     <<< features_path: /home/s3705609/data1/VATEX/clips
03/21/2025 15:57:44 - INFO -     <<< fp16: True
03/21/2025 15:57:44 - INFO -     <<< fp16_opt_level: O2
03/21/2025 15:57:44 - INFO -     <<< freeze_layer_num: 0
03/21/2025 15:57:44 - INFO -     <<< gradient_accumulation_steps: 1
03/21/2025 15:57:44 - INFO -     <<< hard_negative_rate: 0.5
03/21/2025 15:57:44 - INFO -     <<< init_model: None
03/21/2025 15:57:44 - INFO -     <<< linear_patch: 2d
03/21/2025 15:57:44 - INFO -     <<< local_rank: 0
03/21/2025 15:57:44 - INFO -     <<< loose_type: True
03/21/2025 15:57:44 - INFO -     <<< lr: 0.0001
03/21/2025 15:57:44 - INFO -     <<< lr_decay: 0.9
03/21/2025 15:57:44 - INFO -     <<< margin: 0.1
03/21/2025 15:57:44 - INFO -     <<< max_frames: 12
03/21/2025 15:57:44 - INFO -     <<< max_words: 32
03/21/2025 15:57:44 - INFO -     <<< n_display: 50
03/21/2025 15:57:44 - INFO -     <<< n_gpu: 2
03/21/2025 15:57:44 - INFO -     <<< n_pair: 1
03/21/2025 15:57:44 - INFO -   CUDA_VISIBLE_DEVICES: 0,1
03/21/2025 15:57:44 - INFO -     <<< negative_weighting: 1
03/21/2025 15:57:44 - INFO -     <<< num_thread_reader: 2
03/21/2025 15:57:44 - INFO -     <<< output_dir: ckpts3/xclip_vatex_vit16
03/21/2025 15:57:44 - INFO -     <<< pretrained_clip_name: ViT-B/16
03/21/2025 15:57:44 - INFO -   Total available GPUs: 2
03/21/2025 15:57:44 - INFO -     <<< rank: 0
03/21/2025 15:57:44 - INFO -   Process 188822 using device: cuda:1, local_rank: 1
03/21/2025 15:57:44 - INFO -     <<< resume_model: None
03/21/2025 15:57:44 - INFO -     <<< sampled_use_mil: False
03/21/2025 15:57:44 - INFO -   Current device index: 1
03/21/2025 15:57:44 - INFO -     <<< seed: 42
03/21/2025 15:57:44 - INFO -     <<< sim_header: seqTransf
03/21/2025 15:57:44 - INFO -     <<< slice_framepos: 2
03/21/2025 15:57:44 - INFO -   Device properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10824MB, multi_processor_count=68)
03/21/2025 15:57:44 - INFO -     <<< task_type: retrieval
03/21/2025 15:57:44 - INFO -     <<< text_num_hidden_layers: 12
03/21/2025 15:57:44 - INFO -     <<< train_csv: data/.train.csv
03/21/2025 15:57:44 - INFO -     <<< train_frame_order: 0
03/21/2025 15:57:44 - INFO -     <<< use_mil: False
03/21/2025 15:57:44 - INFO -     <<< val_csv: data/.val.csv
03/21/2025 15:57:44 - INFO -     <<< video_dim: 1024
03/21/2025 15:57:44 - INFO -     <<< visual_num_hidden_layers: 12
03/21/2025 15:57:44 - INFO -     <<< warmup_proportion: 0.1
03/21/2025 15:57:44 - INFO -     <<< world_size: 2
03/21/2025 15:57:44 - INFO -   CUDA_VISIBLE_DEVICES: 0,1
03/21/2025 15:57:44 - INFO -   Total available GPUs: 2
03/21/2025 15:57:44 - INFO -   Process 188821 using device: cuda:0, local_rank: 0
03/21/2025 15:57:44 - INFO -   Current device index: 0
03/21/2025 15:57:44 - INFO -   Device properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10824MB, multi_processor_count=68)
03/21/2025 15:57:45 - INFO -   loading archive file /home/s3705609/X-CLIP/modules/cross-base
03/21/2025 15:57:45 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

03/21/2025 15:57:45 - INFO -   Weight doesn't exsits. /home/s3705609/X-CLIP/modules/cross-base/cross_pytorch_model.bin
03/21/2025 15:57:45 - WARNING -   Stage-One:True, Stage-Two:False
03/21/2025 15:57:45 - WARNING -   Test retrieval by loose type.
03/21/2025 15:57:45 - WARNING -   	 embed_dim: 512
03/21/2025 15:57:45 - WARNING -   	 image_resolution: 224
03/21/2025 15:57:45 - WARNING -   	 vision_layers: 12
03/21/2025 15:57:45 - WARNING -   	 vision_width: 768
03/21/2025 15:57:45 - WARNING -   	 vision_patch_size: 16
03/21/2025 15:57:45 - WARNING -   	 context_length: 77
03/21/2025 15:57:45 - WARNING -   	 vocab_size: 49408
03/21/2025 15:57:45 - WARNING -   	 transformer_width: 512
03/21/2025 15:57:45 - WARNING -   	 transformer_heads: 8
03/21/2025 15:57:45 - WARNING -   	 transformer_layers: 12
03/21/2025 15:57:45 - WARNING -   		 linear_patch: 2d
03/21/2025 15:57:45 - WARNING -   	 cut_top_layer: 0
03/21/2025 15:57:49 - WARNING -   	 sim_header: seqTransf
03/21/2025 15:58:00 - INFO -   --------------------
03/21/2025 15:58:00 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
03/21/2025 15:58:00 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
03/21/2025 15:58:12 - INFO -   ***** Running test *****
03/21/2025 15:58:12 - INFO -     Num examples = 60000
03/21/2025 15:58:12 - INFO -     Batch size = 64
03/21/2025 15:58:12 - INFO -     Num steps = 938
03/21/2025 15:58:12 - INFO -   ***** Running val *****
03/21/2025 15:58:12 - INFO -     Num examples = 30000
Total Paire: train 259910
Video number: 23778
Total Paire: train 259910
Total Paire: train 259910
Video number: 23778
Total Paire: train 259910
03/21/2025 15:58:31 - INFO -   ***** Running training *****
03/21/2025 15:58:31 - INFO -     Num examples = 259910
03/21/2025 15:58:31 - INFO -     Batch size = 16
03/21/2025 15:58:31 - INFO -     Num steps = 81220
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Traceback (most recent call last):
  File "/home/s3705609/X-CLIP/main_xclip.py", line 608, in <module>
    main()
  File "/home/s3705609/X-CLIP/main_xclip.py", line 582, in main
    tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/main_xclip.py", line 318, in train_epoch
    for step, batch in enumerate(train_dataloader):
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1325, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
KeyError: Caught KeyError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/s3705609/X-CLIP/dataloaders/dataloader_vatex_neg_aug_word_phrase_retrieval.py", line 421, in __getitem__
    video, video_mask = self._get_rawvideo(choice_video_ids)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/s3705609/X-CLIP/dataloaders/dataloader_vatex_neg_aug_word_phrase_retrieval.py", line 365, in _get_rawvideo
    video_path = self.video_dict[video_id]
                 ~~~~~~~~~~~~~~~^^^^^^^^^^
KeyError: 'j80RGZR2Rzs_000011_000021'

[2025-03-21 15:59:25,463] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 188822 closing signal SIGTERM
[2025-03-21 15:59:25,577] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 188821) of binary: /data1/s3705609/venv_clip/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 810, in <module>
    main()
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/easybuild/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-21_15:59:25
  host      : node855.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 188821)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
